{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzrjhcge1jtlw//DdanvxI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thayane25/PLN/blob/main/Aula_04_Extra%C3%A7%C3%A3o_de_Caracter%C3%ADsticas_(Features)_em_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aula 04 - Extração de característica\n",
        "###O objetivo desse notebook é demonstrar a extração de características (features) de textos por meio de técnicas de representação numérica, como Bag of Words (BoW) e TF-IDF. Utilizando exemplos práticos, o notebook mostra como transformar documentos textuais em vetores numéricos que representam a frequência e a relevância das palavras em um corpus, com o uso de bibliotecas como `scikit-learn`."
      ],
      "metadata": {
        "id": "J42lYwvA_0qb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo 01 - Implementando BoW"
      ],
      "metadata": {
        "id": "pT8YwYwUPuYu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B7aU5xYtPm4",
        "outputId": "dc1e6cc7-6e53-498e-ddc9-68ecb86c29c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulário: {'gato': 3, 'cachorro': 1, 'brinca': 0, 'com': 2, 'rato': 4}\n",
            "Matriz BoW:\n",
            "[[0 1 0 1 0]\n",
            " [1 1 1 1 0]\n",
            " [0 0 0 1 1]]\n"
          ]
        }
      ],
      "source": [
        "# importando a ferramenta que irá criar a representação numérica\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# criando um corpus de documentos que será usado para criação do vocabulário\n",
        "documentos=[\n",
        "      \"gato e cachorro\",\n",
        "      \"gato brinca com cachorro\",\n",
        "      \"gato e rato\"\n",
        "]\n",
        "\n",
        "# criando um objeto para ser utilizado: transformar os documentos em vetores\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# criando a matriz de contagem\n",
        "X = vectorizer.fit_transform(documentos)\n",
        "    # fit >>> cria um vocabulario das palavras\n",
        "    # transforme >>> conta a frequência de cada palavra no corpus\n",
        "\n",
        "# Imprimindo a Matriz e o vocabulário gerado\n",
        "print(f\"Vocabulário: {vectorizer.vocabulary_}\")\n",
        "# realize o mapeamento do vocabulário para o índice da matriz\n",
        "\n",
        "# Corrected indentation for the following lines\n",
        "print(\"Matriz BoW:\")\n",
        "print(X.toarray())\n",
        "\n",
        "# mostra a frequência de cada palavra dentro da matriz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo 02 - Implementando BOW junto com TF-IDF"
      ],
      "metadata": {
        "id": "HQASb5lgSWTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando as Bibliotecas\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "  # Classe que transforma os documentos em vetores e realiza a contagem de frequencia\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "  # Classe que transforma os documentos em vetores e uma contagem de frequencia ponderada\n",
        "\n",
        "# Definindo o corpus\n",
        "documentos = [\n",
        "    \"O cachorro gosta de passear no parque\",\n",
        "    \"O gato dorme no sofá o dia todo\",\n",
        "    \"Cachorros e Gatos podem ser bons amigos\"\n",
        "]\n",
        "\n",
        "#Criando o modelo BoW\n",
        "vectorizer_bow = CountVectorizer()\n",
        "  # Instanciamento da classe em objeto para ser usado\n",
        "X_bow = vectorizer_bow.fit_transform(documentos)\n",
        "  # fit >>> realizar a transformação do vocabulário\n",
        "  # transform >>> transforma cada vetor em um documento com a contagem de frequência\n",
        "\n",
        "# Imprimindo o Vocabulário e a Matriz\n",
        "print (f\"Vocabulário BoW: {vectorizer_bow.vocabulary_}\")\n",
        "print (\"Matriz BoW:\")\n",
        "print (X_bow.toarray())\n",
        "\n",
        "# Criando um modelo TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer()\n",
        "  # Realiza a instanciação da classe em objeto\n",
        "X_tfidf = vectorizer_tfidf.fit_transform(documentos)\n",
        "  # fit >>> realizar a transformação do vocabulário\n",
        "  # transform >>> transforma cada vetor em um documento com a contagem de frequência ponderada\n",
        "\n",
        "# Imprimindo o Vocabulário e a Matriz TF-IDF\n",
        "print (f\"\\nVocabulário TF-IDF: {vectorizer_tfidf.vocabulary_}\")\n",
        "print (\"Matriz TF-IDF\")\n",
        "print (X_tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuyjqSpJPhwp",
        "outputId": "7388236a-f414-4f08-a38f-017362aa0918"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulário BoW: {'cachorro': 2, 'gosta': 9, 'de': 4, 'passear': 12, 'no': 10, 'parque': 11, 'gato': 7, 'dorme': 6, 'sofá': 15, 'dia': 5, 'todo': 16, 'cachorros': 3, 'gatos': 8, 'podem': 13, 'ser': 14, 'bons': 1, 'amigos': 0}\n",
            "Matriz BoW:\n",
            "[[0 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0]\n",
            " [0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 1 1]\n",
            " [1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0]]\n",
            "\n",
            "Vocabulário TF-IDF: {'cachorro': 2, 'gosta': 9, 'de': 4, 'passear': 12, 'no': 10, 'parque': 11, 'gato': 7, 'dorme': 6, 'sofá': 15, 'dia': 5, 'todo': 16, 'cachorros': 3, 'gatos': 8, 'podem': 13, 'ser': 14, 'bons': 1, 'amigos': 0}\n",
            "Matriz TF-IDF\n",
            "[[0.         0.         0.42339448 0.         0.42339448 0.\n",
            "  0.         0.         0.         0.42339448 0.32200242 0.42339448\n",
            "  0.42339448 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.42339448\n",
            "  0.42339448 0.42339448 0.         0.         0.32200242 0.\n",
            "  0.         0.         0.         0.42339448 0.42339448]\n",
            " [0.40824829 0.40824829 0.         0.40824829 0.         0.\n",
            "  0.         0.         0.40824829 0.         0.         0.\n",
            "  0.         0.40824829 0.40824829 0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo 03 - Realizando o pré-processamento e a extração de características do texto"
      ],
      "metadata": {
        "id": "0K1rUo-0VVi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "NxSpwZ122mYE",
        "outputId": "56adebcf-f1cb-4fc7-c15c-398e6cfe4e21"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "smart_open",
                  "wrapt"
                ]
              },
              "id": "463d21675785433da8229a4506dfd8bd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "corpus = [\n",
        "    [\"o\", \"cachorro\", \"está\", \"latindo\", \"no\", \"quintal\"],\n",
        "    [\"o\", \"gato\", \"está\", \"miando\", \"no\", \"telhado\"],\n",
        "    [\"o\", \"pássaro\", \"está\", \"voando\", \"no\", \"céu\"],\n",
        "    [\"a\", \"bola\", \"está\", \"rolando\", \"no\", \"chão\"],\n",
        "    [\"a\", \"criança\", \"está\", \"brincando\", \"com\", \"o\", \"cachorro\"],\n",
        "    [\"o\", \"gato\", \"e\", \"o\", \"rato\", \"são\", \"inimigos\"],\n",
        "    [\"a\", \"água\", \"está\", \"quente\", \"na\", \"caneca\"],\n",
        "    [\"o\", \"sol\", \"está\", \"brilhando\", \"no\", \"céu\"],\n",
        "    [\"a\", \"lua\", \"está\", \"cheia\", \"hoje\", \"no\", \"céu\"],\n",
        "    [\"o\", \"computador\", \"está\", \"ligado\", \"na\", \"mesa\"],\n",
        "    [\"a\", \"lua\", \"está\", \"no\", \"céu\", \"lua\", \"bonita\"]\n",
        "]\n",
        "\n",
        "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "#calculando a similaridade entre palavras acima\n",
        "print(f\"Similaridade entre cachorro e gato: {model.wv.similarity('cachorro','gato')}\")\n",
        "print(f\"Similaridade entre cachorro e bola: {model.wv.similarity('cachorro','bola')}\")\n",
        "print(f\"Similaridade entre céu e lua: {model.wv.similarity('céu','lua')}\")\n",
        "print(f\"Similaridade entre computador e mesa: {model.wv.similarity('computador','mesa')}\")"
      ],
      "metadata": {
        "id": "fAnh1OcXVZQ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2301d7d-d440-47a4-8640-76a8e086a203"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similaridade entre cachorro e gato: 0.004570882301777601\n",
            "Similaridade entre cachorro e bola: -0.08380723744630814\n",
            "Similaridade entre céu e lua: 0.13900601863861084\n",
            "Similaridade entre computador e mesa: -0.1019936203956604\n"
          ]
        }
      ]
    }
  ]
}