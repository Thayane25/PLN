{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thayane25/PLN/blob/main/Aula_07_Descobertas_de_Conhecimentos_em_Textos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Aula 7 - Descoberta de Conhecimento em textos\n",
        "###O objetivo é entender os conceitos de Descoberta de Conhecimento em Textos  e aplicar técnicas como a Identificação de Entidades Nomeadas para extrair informações relevantes. Além disso, busca-se explorar métodos de extração de informações e mineração de textos para gerar insights a partir de grandes volumes de dados textuais."
      ],
      "metadata": {
        "id": "kenSNmrxw86S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo 1 - NER com Spacy"
      ],
      "metadata": {
        "id": "9jrDL30oy9bn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mPRv4RjqQtN5",
        "outputId": "23346e9f-927c-4416-e1ef-f9e68c814e4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting pt-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting pt-core-news-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_md-3.8.0/pt_core_news_md-3.8.0-py3-none-any.whl (42.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pt-core-news-md\n",
            "Successfully installed pt-core-news-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting pt-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.8.0/pt_core_news_lg-3.8.0-py3-none-any.whl (568.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pt-core-news-lg\n",
            "Successfully installed pt-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Instala a biblioteca spaCy, que é usada para Processamento de Linguagem Natural (NLP)\n",
        "!pip install spacy\n",
        "\n",
        "# Baixa o modelo pequeno (sm - small) de português para o spaCy\n",
        "# Esse modelo é mais leve e rápido, ideal para tarefas simples\n",
        "!python -m spacy download pt_core_news_sm\n",
        "\n",
        "# Baixa o modelo médio (md - medium) de português\n",
        "# Ele oferece um equilíbrio entre velocidade e precisão\n",
        "!python -m spacy download pt_core_news_md\n",
        "\n",
        "# Baixa o modelo grande (lg - large) de português\n",
        "# Esse modelo é mais completo e preciso, mas ocupa mais memória\n",
        "!python -m spacy download pt_core_news_lg\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Carrega o modelo em PT\n",
        "nlp = spacy.load(\"pt_core_news_lg\")\n",
        "\n",
        "#Texto de exemplo\n",
        "texto = \"Elon Musk, CEO da Tesla, visitou o Brasil em maio de 2022 para discutir investimentos em R$ 5 bilhões.\"\n",
        "\n",
        "#Processa o texto\n",
        "doc = nlp(texto)\n",
        "\n",
        "#Imprime as entidades identificadas\n",
        "for entidade in doc.ents:\n",
        "  print(f\"{entidade.text} - {entidade.label_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70m94ozywyR7",
        "outputId": "8da73c2c-e9ae-4799-bb17-d92b71f54cc8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon Musk - PER\n",
            "CEO da Tesla - PER\n",
            "Brasil - LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo 02 - NER com NLTK"
      ],
      "metadata": {
        "id": "uoBdHsxnz_X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "\n",
        "#Baixa pacotes necessários\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "#Texto de exemplo\n",
        "texto = \"Barack Obama foi o presidente dos Estados Unidos e ganhou o Premio Nobel da Paz.\"\n",
        "\n",
        "#Tokenização e POS Tagging\n",
        "tokens = word_tokenize(texto)\n",
        "print(tokens)\n",
        "tags = pos_tag(tokens)\n",
        "print(tags)\n",
        "\n",
        "#Identificaçãp de entidades\n",
        "entidades = ne_chunk(tags)\n",
        "\n",
        "#Exibir as entidades reconhecidas\n",
        "\n",
        "print(entidades)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyL_hI4cz-wV",
        "outputId": "5b05c3aa-3932-4dd3-b64f-b68416f4b566"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Barack', 'Obama', 'foi', 'o', 'presidente', 'dos', 'Estados', 'Unidos', 'e', 'ganhou', 'o', 'Premio', 'Nobel', 'da', 'Paz', '.']\n",
            "[('Barack', 'NNP'), ('Obama', 'NNP'), ('foi', 'NN'), ('o', 'NN'), ('presidente', 'NN'), ('dos', 'NN'), ('Estados', 'NNP'), ('Unidos', 'NNP'), ('e', 'NN'), ('ganhou', 'NN'), ('o', 'NN'), ('Premio', 'NNP'), ('Nobel', 'NNP'), ('da', 'NN'), ('Paz', 'NNP'), ('.', '.')]\n",
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (ORGANIZATION Obama/NNP)\n",
            "  foi/NN\n",
            "  o/NN\n",
            "  presidente/NN\n",
            "  dos/NN\n",
            "  (PERSON Estados/NNP Unidos/NNP)\n",
            "  e/NN\n",
            "  ganhou/NN\n",
            "  o/NN\n",
            "  Premio/NNP\n",
            "  Nobel/NNP\n",
            "  da/NN\n",
            "  Paz/NNP\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo 03 - Extração de Informações com Expressões Regulares"
      ],
      "metadata": {
        "id": "PMZUEVju2IrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importa a biblioteca 're', que serve para trabalhar com expressões regulares em Python\n",
        "import re\n",
        "\n",
        "# Texto de exemplo\n",
        "texto = \"O pagamento deve ser feito até 25 de maio de 2025.\"\n",
        "\n",
        "padrao = r\"\\d{1,2} de [a-zA-Z]+ de \\d{4}\"\n",
        "\n",
        "# Usa a função 'findall' para buscar todas as ocorrências no texto que seguem o padrão definido\n",
        "datas = re.findall(padrao, texto)\n",
        "\n",
        "# Imprime a lista de datas encontradas (nesse caso, só uma)\n",
        "print(datas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPK7xkik2Tt_",
        "outputId": "0264b181-d061-41fb-a5fe-23a9fbf53ad3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['25 de maio de 2025']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo 04 - Extração de informação com Regras herísticas e Dicionários"
      ],
      "metadata": {
        "id": "yf0-3qrx3kh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista com algumas profissões para serem buscadas no texto\n",
        "profissoes = [\"engenheiro\", \"cientista de dados\", \"médico\", \"advogado\"]\n",
        "\n",
        "# Texto de exemplo\n",
        "texto = \"Pedro é engenheiro de software e trabalha no Google.\"\n",
        "\n",
        "# Percorre cada profissão da lista\n",
        "for profissao in profissoes:\n",
        "    # Verifica se a profissão atual está presente dentro do texto\n",
        "    if profissao in texto:\n",
        "        # Se encontrar, imprime a profissão detectada\n",
        "        print(f\"Profissão encontrada: {profissao}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djBEl4FO3xPb",
        "outputId": "6e74e31e-b7dc-4ec2-a00a-76912521ea20"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profissão encontrada: engenheiro\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo 05 - Mineração de textos com frequência de palavras e N-gramas"
      ],
      "metadata": {
        "id": "MA_ejNK742DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "texto = \"O bobo é capaz de ficar sentado, quase sem se mexer por duas horas. Se perguntando por que não faz alguma coisa, responde: Estou fazendo. Estou pensando.\"\n",
        "palavras = nltk.word_tokenize(texto.lower())\n",
        "\n",
        "frequencia = Counter(palavras)\n",
        "print(frequencia.most_common(5))\n",
        "\n",
        "bigramas = list(ngrams(palavras, 2))\n",
        "print(bigramas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tztJj7oV5Cz4",
        "outputId": "5b0bf31e-2d8e-4225-f27c-7f750003f975"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('.', 3), (',', 2), ('se', 2), ('por', 2), ('estou', 2)]\n",
            "[('o', 'bobo'), ('bobo', 'é'), ('é', 'capaz'), ('capaz', 'de'), ('de', 'ficar'), ('ficar', 'sentado'), ('sentado', ','), (',', 'quase'), ('quase', 'sem'), ('sem', 'se'), ('se', 'mexer'), ('mexer', 'por'), ('por', 'duas'), ('duas', 'horas'), ('horas', '.'), ('.', 'se'), ('se', 'perguntando'), ('perguntando', 'por'), ('por', 'que'), ('que', 'não'), ('não', 'faz'), ('faz', 'alguma'), ('alguma', 'coisa'), ('coisa', ','), (',', 'responde'), ('responde', ':'), (':', 'estou'), ('estou', 'fazendo'), ('fazendo', '.'), ('.', 'estou'), ('estou', 'pensando'), ('pensando', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo 06 - Mineração de texto"
      ],
      "metadata": {
        "id": "o9WflNZu690Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "9NZ20Osk69ei",
        "outputId": "2fa0e7fb-593c-480b-e2c5-99ec0ea9a3f4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "smart_open",
                  "wrapt"
                ]
              },
              "id": "f060e5f02f8640b689f70f3e6ab16b20"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models\n",
        "\n",
        "documentos = [[\"mineração\", \"textos\", \"dados\"],\n",
        "              [\"inteligência\", \"artificial\", \"aprendizado\"],\n",
        "              [\"dados\", \"aprendizado\", \"máquina\"]]\n",
        "\n",
        "dicionario = corpora.Dictionary(documentos)\n",
        "corpus = [dicionario.doc2bow(documento) for documento in documentos]\n",
        "\n",
        "lda_modelo = models.LdaModel(corpus, num_topics=4, id2word=dicionario)\n",
        "\n",
        "print(lda_modelo.print_topics())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LiMoavR7aiH",
        "outputId": "aed69b4e-7b3c-482b-88c2-73b6a342a42c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '0.262*\"aprendizado\" + 0.259*\"artificial\" + 0.259*\"inteligência\" + 0.057*\"dados\" + 0.055*\"máquina\" + 0.055*\"mineração\" + 0.055*\"textos\"'), (1, '0.146*\"aprendizado\" + 0.145*\"dados\" + 0.142*\"mineração\" + 0.142*\"máquina\" + 0.141*\"artificial\" + 0.141*\"textos\" + 0.141*\"inteligência\"'), (2, '0.148*\"inteligência\" + 0.146*\"dados\" + 0.145*\"aprendizado\" + 0.144*\"artificial\" + 0.139*\"textos\" + 0.139*\"mineração\" + 0.139*\"máquina\"'), (3, '0.288*\"dados\" + 0.162*\"aprendizado\" + 0.161*\"textos\" + 0.161*\"máquina\" + 0.161*\"mineração\" + 0.034*\"artificial\" + 0.033*\"inteligência\"')]\n"
          ]
        }
      ]
    }
  ]
}